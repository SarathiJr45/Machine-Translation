{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1NzkiJW7reNY1FL4xuODzQ637PsP40oJA",
      "authorship_tag": "ABX9TyMN8hIbUR0WEVgs7nkbm5JV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SarathiJr45/Machine-Translation/blob/main/Machine_Translation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "s08eprMdhZh5"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_sentences(file_path):\n",
        "  sentences=[]\n",
        "  with open(file_path,'r',encoding='utf-8') as file:\n",
        "    for line in file:\n",
        "      sentence=line.strip()\n",
        "      if sentence:\n",
        "        sentences.append(sentence)\n",
        "    return sentences\n",
        "\n",
        "def load_bilingual_data(source_lang,target_lang):\n",
        "  source_sentences=load_sentences(source_lang)\n",
        "  target_sentences=load_sentences(target_lang)\n",
        "\n",
        "  assert len(source_sentences)==len(target_sentences),'mismatched no.of sentences'\n",
        "\n",
        "  alligned_data=list(zip(source_sentences,target_sentences))\n",
        "  return alligned_data\n",
        "\n",
        "source_file_path='/content/drive/MyDrive/en-ta.txt/GNOME.en-ta.en'\n",
        "target_file_path='/content/drive/MyDrive/en-ta.txt/GNOME.en-ta.ta'\n",
        "\n",
        "bilingual_data=load_bilingual_data(source_file_path,target_file_path)\n",
        "print(len(bilingual_data))\n",
        "print(bilingual_data[100])"
      ],
      "metadata": {
        "id": "xwcbDNOBhxHh"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#preprocess\n",
        "import string\n",
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "\n",
        "def preprocess_sentences(sentence):\n",
        "  sentence=sentence.lower()\n",
        "  sentence=sentence.translate(str.maketrans('','',string.punctuation))\n",
        "  sentence=re.sub(r'\\d+','',sentence)\n",
        "  tokens=word_tokenize(sentence)\n",
        "  return tokens\n",
        "\n",
        "\n",
        "def preprocess_bilingual_data(bilingual_data):\n",
        "  preprocessed_data=[]\n",
        "  for source_sentence,target_sentence in bilingual_data:\n",
        "    source_tokens=preprocess_sentences(source_sentence)\n",
        "    target_tokens=preprocess_sentences(target_sentence)\n",
        "    preprocessed_data.append((source_tokens,target_tokens))\n",
        "  return preprocessed_data\n",
        "\n",
        "\n",
        "preprocessed_bilingual_data=preprocess_bilingual_data(bilingual_data)\n",
        "\n"
      ],
      "metadata": {
        "id": "He8cmqNgkXDT"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print( len(preprocessed_bilingual_data))\n",
        "print(preprocessed_bilingual_data[1000])\n",
        "print(bilingual_data[1000])"
      ],
      "metadata": {
        "id": "GChL9Rq1xHKX"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the preprocessed data into train, validation, and test sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_size=0.8\n",
        "train_data,temp_data=train_test_split(preprocessed_bilingual_data,train_size=0.8,random_state=42)\n",
        "remaining_size=1-train_size\n",
        "val_data,test_data=train_test_split(temp_data,test_size=0.1/remaining_size,random_state=42)\n",
        "print(len(train_data))\n",
        "print(len(val_data))\n",
        "print(len(test_data))"
      ],
      "metadata": {
        "id": "zU6zsOKy0QnP"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "def tokenize_data(data):\n",
        "\n",
        "    source_sentences = [source for source, _ in data]\n",
        "    target_sentences = [target for _, target in data]\n",
        "\n",
        "    source_tokenizer = Tokenizer(filters='')\n",
        "    source_tokenizer.fit_on_texts(source_sentences)\n",
        "    source_sequences = source_tokenizer.texts_to_sequences(source_sentences)\n",
        "\n",
        "\n",
        "    target_tokenizer = Tokenizer(filters='')\n",
        "    target_sentences_with_tokens = ['<start> ' + str(sentence) + ' <end>' for sentence in target_sentences]\n",
        "    target_tokenizer.fit_on_texts(target_sentences)\n",
        "    target_word_index = target_tokenizer.word_index\n",
        "\n",
        "\n",
        "    target_sequences = target_tokenizer.texts_to_sequences(target_sentences)\n",
        "\n",
        "    return source_tokenizer, target_tokenizer, source_sequences, target_sequences\n",
        "\n",
        "def pad_data(source_sequences, target_sequences, max_length):\n",
        "    \"\"\"\n",
        "    Pad the sequences to ensure uniform length.\n",
        "    \"\"\"\n",
        "    padded_source_sequences = pad_sequences(source_sequences, maxlen=max_length, padding='post')\n",
        "    padded_target_sequences = pad_sequences(target_sequences, maxlen=max_length, padding='post')\n",
        "    return padded_source_sequences, padded_target_sequences\n",
        "\n",
        "\n",
        "source_tokenizer, target_tokenizer, source_sequences, target_sequences = tokenize_data(train_data)\n",
        "\n",
        "\n",
        "max_length = 50\n",
        "padded_source_sequences, padded_target_sequences = pad_data(source_sequences, target_sequences, max_length)\n",
        "\n",
        "\n",
        "print( padded_source_sequences.shape)\n",
        "print( padded_target_sequences.shape)\n"
      ],
      "metadata": {
        "id": "Wilo2ecbzJCY"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding, Attention\n",
        "\n",
        "\n",
        "def build_model(source_vocab_size, target_vocab_size, max_length):\n",
        "\n",
        "    encoder_inputs = Input(shape=(max_length,))\n",
        "    encoder_embedding = Embedding(source_vocab_size, 256, input_length=max_length)(encoder_inputs)\n",
        "    encoder_outputs, state_h, state_c = LSTM(256, return_sequences=True, return_state=True)(encoder_embedding)\n",
        "    encoder_states = [state_h, state_c]\n",
        "\n",
        "\n",
        "    decoder_inputs = Input(shape=(None,))\n",
        "    decoder_embedding = Embedding(target_vocab_size, 256)(decoder_inputs)\n",
        "    decoder_lstm = LSTM(256, return_sequences=True, return_state=True)\n",
        "    decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
        "\n",
        "\n",
        "    attention = Attention()\n",
        "    context_vector = attention([decoder_outputs, encoder_outputs])\n",
        "\n",
        "\n",
        "    decoder_combined_context = tf.concat([decoder_outputs, context_vector], axis=-1)\n",
        "\n",
        "\n",
        "    decoder_dense = Dense(target_vocab_size, activation='softmax')\n",
        "    decoder_outputs = decoder_dense(decoder_combined_context)\n",
        "\n",
        "\n",
        "    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "    return model\n",
        "\n",
        "source_vocab_size = len(source_tokenizer.word_index) + 1\n",
        "target_vocab_size = len(target_tokenizer.word_index) + 1\n",
        "\n",
        "\n",
        "model = build_model(source_vocab_size, target_vocab_size, max_length)\n",
        "\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "OwiPGF-TUlrG"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_val_data(data):\n",
        "\n",
        "    source_sentences = [source for source, _ in data]\n",
        "    target_sentences = [target for _, target in data]\n",
        "\n",
        "\n",
        "    source_tokenizer = Tokenizer(filters='')\n",
        "    source_tokenizer.fit_on_texts(source_sentences)\n",
        "    source_sequences = source_tokenizer.texts_to_sequences(source_sentences)\n",
        "\n",
        "    target_tokenizer = Tokenizer(filters='')\n",
        "    target_tokenizer.fit_on_texts(target_sentences)\n",
        "    target_sequences = target_tokenizer.texts_to_sequences(target_sentences)\n",
        "\n",
        "    return source_sequences, target_sequences\n",
        "\n",
        "\n",
        "\n",
        "train_source_sequences, train_target_sequences = padded_source_sequences, padded_target_sequences\n",
        "\n",
        "\n",
        "val_source_sequences, val_target_sequences = tokenize_val_data(val_data)\n",
        "val_padded_source_sequences, val_padded_target_sequences = pad_data(val_source_sequences, val_target_sequences, max_length)\n",
        "\n",
        "history = model.fit(\n",
        "    [train_source_sequences, train_target_sequences[:, :-1]],\n",
        "    train_target_sequences[:, 1:],\n",
        "    validation_data=([val_padded_source_sequences, val_padded_target_sequences[:, :-1]],\n",
        "                     val_padded_target_sequences[:, 1:]),\n",
        "    batch_size=64,\n",
        "    epochs=10\n",
        ")\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "sPNmMYCJrOgA"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "test_source_sequences, test_target_sequences = tokenize_val_data(test_data)\n",
        "test_padded_source_sequences, test_padded_target_sequences = pad_data(test_source_sequences, test_target_sequences, max_length)\n",
        "\n",
        "test_loss, test_accuracy = model.evaluate([test_padded_source_sequences, test_padded_target_sequences[:, :-1]],\n",
        "                                          test_padded_target_sequences[:, 1:])\n",
        "\n",
        "print(\"Test Loss:\", test_loss)\n",
        "print(\"Test Accuracy:\", test_accuracy)"
      ],
      "metadata": {
        "id": "keJnqj9505c8"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def translate_sentence(sentence, source_tokenizer, target_tokenizer, model, max_length):\n",
        "\n",
        "    sentence = preprocess_sentence(sentence)\n",
        "    sequence = source_tokenizer.texts_to_sequences([sentence])[0]\n",
        "    sequence = pad_sequences([sequence], maxlen=max_length, padding='post')\n",
        "    target_sequence = np.zeros((1, max_length))\n",
        "    target_sequence[0, 0] = target_tokenizer.word_index['<start>']\n",
        "\n",
        "    for i in range(max_length):\n",
        "        if i > 0:\n",
        "            prediction = model.predict([sequence, target_sequence])\n",
        "            predicted_index = np.argmax(prediction[0, i - 1, :])\n",
        "            if predicted_index == target_tokenizer.word_index['<end>']:\n",
        "                break\n",
        "            target_sequence[0, i] = predicted_index\n",
        "\n",
        "    translated_sentence = [target_tokenizer.index_word[int(index)] for index in target_sequence[0] if int(index) != 0]\n",
        "    translated_sentence = ' '.join(translated_sentence[1:])\n",
        "    return translated_sentence\n",
        "\n",
        "\n",
        "source_sentence = \"IDistinguished\"\n",
        "translated_sentence = translate_sentence(source_sentence, source_tokenizer, target_tokenizer, model, max_length)\n",
        "print(\"Source Sentence:\", source_sentence)\n",
        "print(\"Translated Sentence:\", translated_sentence)\n"
      ],
      "metadata": {
        "id": "cc-WXzkr05fE"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0Nu1ovtQu6FW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}